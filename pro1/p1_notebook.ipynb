{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import operator\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[1-gram]\n",
      "\n",
      "Empty sentence\n",
      "[1]  From the content of many of your posts you appear to have a lot of useful information to share with people but it gets overshadowed when you come across as an abusive smart ass\n",
      "[2]  I value this net group as one where people focus on solving problems and go out of their way to be respectful of differences\n",
      "[3]  I was wondering if anyone could please tell me what the road rules are in either country with regard to intersections\n",
      "\n",
      "With incompelete sentence: \"I have\"\n",
      "[1]  I have recently purchased within the last 8 months the bel 966stw\n",
      "[2]  I have also heard that they try to get rid of you if you have an accident\n",
      "[3]  I have been told that most of the us assembly plants for japanese automakers import almost all of the parts used in the vehicles\n",
      "\n",
      "\n",
      "[2-gram]\n",
      "\n",
      "Empty sentence\n",
      "[1]  And for a good dose of luxury the lexus sc300 is perfect with a manual transmission of course\n",
      "[2]  In the summer in your 10w40 car\n",
      "[3]  From all i ve heard from my u\n",
      "\n",
      "With incompelete sentence: \"I have\"\n",
      "[1]  I have read that the new 1993 models have a newer improved hp engine\n",
      "[2]  I have no idea how either of my hondas will handle at 100 mph nor do they reach 155\n",
      "[3]  I have one on both of my cars\n",
      "\n",
      "\n",
      "[3-gram]\n",
      "\n",
      "Empty sentence\n",
      "[1]  There is one noise that is expecially irritating the back window squeaks\n",
      "[2]  Anyway you put it you need those fancy scmancy gadgets awww right you want all the home mechanics lined up against a wall and shot eh\n",
      "[3]  Most that go 130 are built to do it also and can handle the speed\n",
      "\n",
      "With incompelete sentence: \"I have\"\n",
      "[1]  I have also heard that they try to get rid of tailgaters if you get that rush of testosterone\n",
      "[2]  I have also heard that they try to get rid of tailgaters if you get that rush of testosterone\n",
      "[3]  I have been told that most of the us assembly plants for japanese automakers import almost all of the parts used in the vehicles\n",
      "\n",
      "\n",
      "[4-gram]\n",
      "\n",
      "Empty sentence\n",
      "[1]  You always believe those exact numbers why don t you go shoot some kids who are tossing rocks onto cars\n",
      "[2]  Spiros spiros triantafyllopoulos software technology delco electronics 317 451 0815 gm hughes electronics kokomo in 46904 I post therefore I armm from chun hung wan subject re i m getting a car I need opinions\n",
      "[3]  Johnson san diego california intermittent newsfeed at best and only to selected groups\n",
      "\n",
      "With incompelete sentence: \"I have\"\n",
      "[1]  I have seen them on quite a few cars but I can t find anything more about them in previous r\n",
      "[2]  I have had all the major brand detectors and imho nothing else even comes close to the v1\n",
      "[3]  I have been having problems with a slightly different clutch problem on my 90 prelude\n",
      "\n",
      "\n",
      "[5-gram]\n",
      "\n",
      "Empty sentence\n",
      "[1]  So now bel has a new series of detectors out that cover all the usual bands x k ka photo ka wideband as well as the new super ka wideband\n",
      "[2]  Plenty off highway marginal on the highway\n",
      "[3]  Any advice on how to buy a land cruiser\n",
      "\n",
      "With incompelete sentence: \"I have\"\n",
      "[1]  I have recently purchased within the last 8 months the bel 966stw\n",
      "[2]  I have also heard that they try to get rid of tailgaters if you get that rush of testosterone\n",
      "[3]  I have had all the major brand detectors and imho nothing else even comes close to the v1\n"
     ]
    }
   ],
   "source": [
    "indir_pre = os.getcwd() + \"/\"\n",
    "outdir_pre = os.getcwd() + \"/\"\n",
    "sentence_maxlen = 100\n",
    "sentence_minlen = 5\n",
    "nprob_dic, nhash_dic, ncounter_dic = {}, {}, {}\n",
    "\n",
    "def preprocess(indir):\n",
    "    # TODO\n",
    "    # Upper-lower case done\n",
    "    # temp = sent_tokenize(indir) done\n",
    "    # output = \"\"\n",
    "    # delete email done\n",
    "    # ...\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "# #     pat = re.compile(r\"\\p{P}+\")\n",
    "#     result = re.findall(r'[\\w]+',text)\n",
    "#     delim = \" \"\n",
    "#     return delim.join(result)\n",
    "\n",
    "    buffer, output = \"\", \"\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for f in filenames:\n",
    "            raw_content = open(os.path.join(root, f),'r').read()\n",
    "            buffer += raw_content\n",
    "    temp = sent_tokenize(buffer)\n",
    "\n",
    "    for sent in temp:\n",
    "        output += \"<s> \" + sent + \" </s> \"\n",
    "    # final = remove_punctuation(output)\n",
    "    final = output\n",
    "    return final\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "#     pat = re.compile(r\"\\p{P}+\")\n",
    "    result = re.findall(r'[\\w]+',text)\n",
    "    delim = \" \"\n",
    "    return delim.join(result)\n",
    "\n",
    "def remove_email(text):\n",
    "    result = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','',text)    \n",
    "    return result\n",
    "\n",
    "def preprocess_jiao(indir):\n",
    "    buffer, output = \"\", \"\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for f in filenames:\n",
    "            raw_content = open(os.path.join(root, f),'r').read()\n",
    "            buffer += raw_content\n",
    "\n",
    "    # normalize\n",
    "    buffer = buffer.lower()\n",
    "    buffer = remove_email(buffer)\n",
    "#     buffer = buffer.replace('-', '')\n",
    "#     buffer = buffer.replace('\\\\', '')\n",
    "#     buffer = buffer.replace('/', '')\n",
    "    buffer = buffer.replace('_', '')\n",
    "#     buffer = buffer.replace('|', '')\n",
    "    buffer = buffer.replace(\"From : \",'')\n",
    "    buffer = buffer.replace(\"Subject : \",'')\n",
    "#     buffer = buffer.replace('(', '')\n",
    "#     buffer = buffer.replace(')', '')\n",
    "#     buffer = buffer.replace('<', '')\n",
    "#     buffer = buffer.replace('>', '')\n",
    "#     buffer = buffer.replace('|', '')\n",
    "#     buffer = buffer.replace('\\\"', '')\n",
    "#     buffer = buffer.replace(',', '')\n",
    "#     buffer = buffer.replace('=', '')\n",
    "#     buffer = buffer.replace('#', '')\n",
    "    buffer = buffer.replace(' i ', ' I ')\n",
    "    buffer = buffer.replace(' i\\' ', ' I\\' ')\n",
    "\n",
    "    temp = sent_tokenize(buffer)\n",
    "\n",
    "    for sent in temp:\n",
    "        sent = remove_punctuation(sent)\n",
    "\n",
    "        output += \" <s> \" + sent + \" </s> \"\n",
    "\n",
    "    final = output\n",
    "    return final\n",
    "\n",
    "\n",
    "def ntoken_count(n, content):\n",
    "    counter = {}\n",
    "    tokens = content.split()\n",
    "    _len = len(tokens)\n",
    "    for i in xrange(_len - n + 1):\n",
    "        key = tuple(tokens[i:(i + n)])\n",
    "        counter[key] = counter.get(key, 0) + 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def ngram_generator(n, content):\n",
    "    ncounter_dic[n] = ncounter_dic[n] if n in ncounter_dic else ntoken_count(n, content)\n",
    "    nhash_dic[n], nprob_dic[n] = {}, {}\n",
    "\n",
    "    if n == 1:\n",
    "        _sum = sum(ncounter_dic[n].values())\n",
    "        nprob_dic[n] = dict((key, num * 1.0 / _sum) for key, num in ncounter_dic[n].items())\n",
    "    elif n > 1:\n",
    "        ncounter_dic[n - 1] = ncounter_dic[n - 1] if n - 1 in ncounter_dic else ntoken_count(n - 1, content)\n",
    "        for key_n, num_n in ncounter_dic[n].items():\n",
    "            key_nminus1 = key_n[:-1]\n",
    "\n",
    "            nhash_dic[n][key_nminus1] = nhash_dic[n].get(key_nminus1, [])\n",
    "            nhash_dic[n][key_nminus1].append(key_n)\n",
    "\n",
    "            num_nminus1 = ncounter_dic[n - 1][key_nminus1]\n",
    "            nprob_dic[n][key_n] = 1.0 * num_n / num_nminus1\n",
    "\n",
    "    return nprob_dic[n]\n",
    "\n",
    "\n",
    "def sentence_generator(n, content, sentence = '<s>'):\n",
    "    normalized_sentence = \"\"\n",
    "    if not sentence.startswith('<s> '):\n",
    "        sentence = '<s> ' + sentence\n",
    "\n",
    "    while len(normalized_sentence) < sentence_minlen:\n",
    "        sentence_list = sentence.split()\n",
    "        while len(sentence_list) < sentence_minlen or \\\n",
    "            (len(sentence_list) < sentence_maxlen and sentence_list[-1] != '</s>'):\n",
    "            sentence_list = backoff_produce_next(min(len(sentence_list) + 1, n), content, sentence_list)\n",
    "\n",
    "        # normalize: remove '<s>', '<\\s>', multiple white spaces\n",
    "        normalized_sentence = ' '.join(sentence_list).replace('<s>', '').replace('</s>', '')\n",
    "        normalized_sentence = ' '.join(normalized_sentence.split())\n",
    "\n",
    "    normalized_sentence = normalized_sentence[0].upper() + normalized_sentence[1:]\n",
    "\n",
    "    return normalized_sentence\n",
    "\n",
    "\n",
    "def backoff_produce_next(n, content, sentence_list):\n",
    "    nprob_dic[n] = nprob_dic[n] if n in nprob_dic else ngram_generator(n, content)\n",
    "    rand_prob = random.uniform(0.0, 1.0)\n",
    "    prob_sum = 0\n",
    "\n",
    "    if n == 1:\n",
    "        for token in nprob_dic[1]:\n",
    "            prob_sum += nprob_dic[1][token]\n",
    "            if prob_sum > rand_prob:\n",
    "                sentence_list.append(token[0])\n",
    "                break\n",
    "    else:\n",
    "        key = tuple(sentence_list[-n+1:])\n",
    "        if key not in nhash_dic[n]:\n",
    "            sentence_list = backoff_produce_next(n - 1, content, sentence_list)\n",
    "        else:\n",
    "            for token in nhash_dic[n][key]:\n",
    "                prob_sum += nprob_dic[n][tuple(token)]\n",
    "                if prob_sum > rand_prob:\n",
    "                    sentence_list.append(token[-1])\n",
    "                    break\n",
    "\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    argv = [\"data/autos/train_docs\", \"5\", \"I have\"] # TODO: input\n",
    "    if (len(argv) == 0):\n",
    "        print \"Please input a topic\"\n",
    "\n",
    "    topic = argv[0]\n",
    "    n = int(argv[1]) if len(argv) > 1 and argv[1].isdigit() \\\n",
    "        and int(argv[1]) >= 1 else 1\n",
    "    sent_pre = argv[2] if len(argv) > 2 else \"\"\n",
    "\n",
    "    indir, outdir = indir_pre + topic, outdir_pre + topic\n",
    "\n",
    "    if not os.path.isdir(indir):\n",
    "        print \"Sorry, the topic does not exist!\"\n",
    "        return\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # content = preprocess(indir)\n",
    "    content = preprocess_jiao(indir)\n",
    "\n",
    "\n",
    "    for k in xrange(1, n + 1):\n",
    "        print \"\\n\\n[{}-gram]\\n\".format(k)\n",
    "\n",
    "        print \"Empty sentence\"\n",
    "        for i in xrange(3):\n",
    "            print \"[{}]  \".format(i + 1) + sentence_generator(n, content)\n",
    "\n",
    "        print \"\\nWith incompelete sentence: \" + \"\\\"{}\\\"\".format(sent_pre)\n",
    "        for i in xrange(3):\n",
    "            print \"[{}]  \".format(i + 1) + sentence_generator(n, content, sent_pre)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
